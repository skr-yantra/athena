% Created by Yantra on 2019-08-15.
% Copyright (c) 2019 .
\documentclass[11pt,a4paper]{article}

\usepackage{titling}
\usepackage[a4paper,left=2cm,right=2cm,top=3cm,bottom=3cm]{geometry}
\usepackage{tabularx}

\setlength{\droptitle}{-7em} 

\title{Evaluating Hierarchical Deep Learning Methods For Vision Based Robot Manipulation Of Dynamic Objects In Unstructured Environments \vspace{-6ex}}
\date{}

\begin{document}

\maketitle

In order to assist in general tasks, autonomous robots should be able to interact with dynamic objects in unstructured environments. Robot manipulation of objects is the key component in all autonomous robot applications requiring interaction with environment. In vision based robot manipulation, robot has to measure the environment state using camera and take actions according the measured state and goal. The main challenges in here are interpreting the noisy high dimensional data from camera and deciding actions according to stochastic and non stationary environment state.

In previous works, methods for robot manipulation can be broadly classified into traditional and data driven methods. In traditional methods, data from camera is interpreted using computer vision algorithms and actions are hand coded based on interpreted state. This approach works well for robots deployed for specific task like in automated production lines. But in stochastic and non stationary environments, it is not possible to hand code actions for all environment states. On the other hand data driven methods, particularly reinforcement learning have shown great potential in this use case.  In reinforcement learning, an agent learns to take actions (policy) based on measured environment state by interacting with the environment through trial and error for maximising a feedback signal (reward or value function). The main challenge in this method is requirement of large amount of data for agent to learn. Collecting large amount of data from real robot will be slow and expensive and might require manual intervention. Instead, for faster and cheaper development, agent can be trained on simulated robot manipulation environment. However policies trained on simulated environments are not directly transferable to real robots due to various differences between simulation and real environments. 

Work done by Michel Breyer et. al. \cite{visual-robot-manipulation} found that using autoencoder to reduce dimensionality of camera data and using curriculum learning reduced the training time of agents. Also by using shaped reward functions instead of sparse reward function, they obtained 98\% success rate on simulated environment. They also found that using RANSAC for detecting and filtering surfaces from camera data while using policies trained from simulated environment on real robot gave 78\% success rate. This project will evaluate the performance of hierarchical reinforcement learning methods by modifying work done by Michel Breyer et. al. \cite{visual-robot-manipulation}. Traditional reinforcement learning methods are data inefficient (require lot of data for training), difficult to scale (due to large action and/or state space) and brittle due to over specialisation (difficult to transfer their experience to new even similar environments) \cite{thegradient}. Hierarchical reinforcement learning are intended to address these issues by learning to operate on different levels of temporal abstraction. Using this method, the entire task of robot manipulation can be split into smaller sub-tasks like reaching, grasping, lifting etc. Project plan is to train and evaluate hierarchical reinforcement learning methods on simulated pick and place robot manipulation environment using bullet physics simulator. Policies learned on simulated environment will be evaluated on 6DOF articulated 2 fingered robot. Pick and place success rates in simulated and real robot will be compared. New method is expected to have better success rate in real robot environment and to be faster to train.

\vspace{10mm}

\begin{tabularx}{\textwidth}{c X c}
\textbf{Guided by} & & \textbf{Submitted by} \\
Linu Shine & & Sreejith Krishnan R (17) \\
Electronics and Communication Engineering & & Robotics and Automation \\
College of Engineering, Trivandrum & & \\
\end{tabularx}

\begin{thebibliography}{9}
\bibitem{visual-robot-manipulation} 
M. Breyer, F. Furrer, T. Novkovic, R. Siegwart and J. Nieto, 
\textit{Comparing Task Simplifications to Learn Closed-Loop Object Picking Using Deep Reinforcement Learning}. 
IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 1549-1556, April 2019

\bibitem{thegradient} 
The Gradient - \textit{The Promise of Hierarchical Reinforcement Learning}
\\\texttt{https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/}

\bibitem{google} 
D. Quillen, E. Jang, O. Nachum, C. Finn, J. Ibarz and S. Levine, 
\textit{Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods}.
IEEE International Conference on Robotics and Automation (ICRA), Brisbane, QLD, 2018, pp. 6284-6291

\bibitem{google1} 
Eric Jang, Coline Devin, Vincent Vanhoucke, Sergey Levine, 
\textit{Grasp2Vec: Learning Object Representations from Self-Supervised Grasping}.
Proceedings of The 2nd Conference on Robot Learning, in PMLR 87:99-112 (2018)
\end{thebibliography}
\end{document}